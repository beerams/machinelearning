{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from corpora_utils import *\n",
    "\n",
    "random.seed(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data from data/stanford_movie_reviews_train\n",
    "# data returned is a list of tuples\n",
    "# each tuple is of the form (label = 1 or -1, dictionary of word counts)\n",
    "train_data = load_stanford_imdb_train_data()\n",
    "\n",
    "# prepare a dictionary of weights, initialized to some small random value\n",
    "weights = {word: 0.0 for _, wc in train_data for word in wc}\n",
    "bias = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define learning parameters\n",
    "n_iterations = 2500\n",
    "eta = 0.01  # learning rate\n",
    "\n",
    "# useful functions\n",
    "def scalar_product(c, x):\n",
    "    \"\"\"\n",
    "    c - a scalar\n",
    "    x - dictionary -- word counts\n",
    "    \"\"\"\n",
    "    return {k: c*x[k] for k in x}\n",
    "\n",
    "def dot_product(dict1, dict2):\n",
    "    if len(dict1) > len(dict2):\n",
    "        return dot_product(dict2, dict1)\n",
    "    dp = 0.0\n",
    "    for k in dict1:\n",
    "        dp += dict1[k]*dict2.get(k, 0.0)\n",
    "    return dp\n",
    "\n",
    "# score function\n",
    "def score(y, x, w, b):\n",
    "    \"\"\"\n",
    "    Score is just an affine function, its calculated as y*(x.w + b)\n",
    "    \"\"\"\n",
    "    return y*(dot_product(x, w) + b)\n",
    "\n",
    "# hinge loss\n",
    "def hinge_loss(y, x, w, b):\n",
    "    \"\"\"\n",
    "    Computes hinge loss is calculated as max{0.0, 1 - score}\n",
    "    \"\"\"\n",
    "    return max(0.0, (1 - score(y, x, w, b)))\n",
    "\n",
    "# gradients of w & b\n",
    "def calc_gradients(y, x, w, b):\n",
    "    \"\"\"\n",
    "    Calculates gradients of weights and bias\n",
    "    Returns a tuple (weights gradient, bias gradient, boolean indicating if weights need to be updated)\n",
    "    \n",
    "    y: float - training sample's label\n",
    "    x: dict - training sample's features\n",
    "    w: dict - weights\n",
    "    b: float - bias\n",
    "    \"\"\"\n",
    "    s = score(y, x, w, b)\n",
    "    return (scalar_product(-y, x), -y, True) if s < 1 else (0.0, 0.0, False)\n",
    "\n",
    "# update weights\n",
    "def update_weights(w, b, w_grad, b_grad, lr):\n",
    "    \"\"\"\n",
    "    w - weights, to be updated. NOTE: weights are updated inplace -- a mutation\n",
    "    b - bias, to be updated\n",
    "    w_grad - weights gradient\n",
    "    b_grad - bias gradient\n",
    "    lr - learning rate\n",
    "    \"\"\"\n",
    "    for k in w_grad:\n",
    "        if k in w:\n",
    "            w[k] -= lr*w_grad[k]\n",
    "    b -= lr*b_grad\n",
    "    return w, b  # though weights are returned, they are actually updated inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss after training = 1.5812760000000006\n"
     ]
    }
   ],
   "source": [
    "total_hl = 0.0\n",
    "for i in range(1, n_iterations+1):\n",
    "    # select a random training sample\n",
    "    label, features = random.choice(train_data)\n",
    "    \n",
    "    # compute loss with current weights and bias\n",
    "    hl = hinge_loss(label, features, weights, bias)\n",
    "    \n",
    "    # calculate gradients with respect to weights and bias\n",
    "    weight_grad, bias_grad, requires_weight_update = calc_gradients(label, features, weights, bias)\n",
    "    \n",
    "    # update weights and bias if required\n",
    "    if requires_weight_update:\n",
    "        weights, bias = update_weights(weights, bias, weight_grad, bias_grad, eta)\n",
    "    \n",
    "    total_hl += hl\n",
    "    # print('Iteration# {} avg. training loss = {}'.format(i, total_hl/i))\n",
    "\n",
    "print('Train loss after training = {}'.format(total_hl/n_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#correct = 3658\n",
      "#wrong = 1342\n",
      "% correct = 73.2\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "test_data = load_stanford_imdb_test_data()\n",
    "\n",
    "n_test = len(test_data)\n",
    "n_correct = 0\n",
    "for y_test, x_test in test_data:\n",
    "    if score(y_test, x_test, weights, bias) > 0:\n",
    "        n_correct += 1\n",
    "\n",
    "print('#correct = {}'.format(n_correct))\n",
    "print('#wrong = {}'.format(n_test - n_correct))\n",
    "print('% correct = {}'.format(round(n_correct*100.0/n_test, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
