{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usages of various PyTorch apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "def print_table(header, rows):\n",
    "    md = ['|' + ' | '.join(header) + '|']\n",
    "    for i, row in enumerate(rows):\n",
    "        if i == 0:\n",
    "            n_cols = len(row)\n",
    "            md.append('|--'*n_cols + '|')\n",
    "        md.append('| {} |'.format(' | '.join([str(cell) for cell in row])))\n",
    "    display_markdown('\\n'.join(md), raw=True)\n",
    "\n",
    "def print_md(s):\n",
    "    display_markdown(s, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "[API Docs](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Sentences represented as array of indices"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*this is my book* => [13, 100, 88, 100]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*those are your books* => [61, 100, 78, 100]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*what is your name* => [11, 100, 78, 100]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*i will be back* => [100, 33, 98, 100]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*go out and about* => [69, 40, 70, 25]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Sentences represented as a tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 13, 100,  88, 100],\n",
      "        [ 61, 100,  78, 100],\n",
      "        [ 11, 100,  78, 100],\n",
      "        [100,  33,  98, 100],\n",
      "        [ 69,  40,  70,  25]])\n",
      "Shape = torch.Size([5, 4])\n"
     ]
    }
   ],
   "source": [
    "# This example illustrates the usage of nn.Embedding\n",
    "# Embedding is a lookup table -- to lookup a vector stored against a key (typically an integer index)\n",
    "# You supply a bunch of keys, you get back corresponding bunch of vectors\n",
    "\n",
    "# This example tries to lookup vectors corresponding to words in a sentence\n",
    "# Setup: here are 100 english words. Index of 'how' is 0, index of 'to' is 1, ... index of 'two' is 99\n",
    "v100 = ['how', 'to', 'her', 'at', 'up', 'see', 'in', 'thing', 'even', 'because', 'or', \n",
    "             'what', 'man', 'this', 'for', 'with', 'time', 'now', 'give', 'very', 'take', 'other', \n",
    "             'there', 'would', 'first', 'about', 'people', 'think', 'find', 'so', 'say', 'as', \n",
    "             'many', 'will', 'just', 'he', 'I', 'well', 'our', 'tell', 'out', 'have', 'can', 'its', \n",
    "             'make', 'get', 'if', 'than', 'use', 'that', 'new', 'also', 'from', 'by', 'his', 'year', \n",
    "             'do', 'some', 'the', 'no', 'a', 'those', 'she', 'come', 'one', 'their', 'more', \n",
    "             'these', 'all', 'go', 'and', 'could', 'him', 'into', 'only', 'who', 'of', 'it', 'your', \n",
    "             'not', 'you', 'here', 'when', 'on', 'which', 'then', 'know', 'them', 'my', 'me', 'we', \n",
    "             'want', 'they', 'like', 'look', 'day', 'way', 'but', 'be', 'two']\n",
    "\n",
    "# Naturally this vocabulary is insufficient even for most common sentences. Map all out of vocabulary words to <unk>\n",
    "# add <unk> to vocabulary at index 100. <unk> is the replacement for out of vocabulary words\n",
    "v100.append('<unk>')\n",
    "\n",
    "# create a dictionary to lookup word's index\n",
    "w2i = {word: i for i, word in enumerate(v100)}\n",
    "\n",
    "# define few sentences of fixed size. we wish to lookup embeddings for words in these sentences\n",
    "# here are 5 sentences\n",
    "sents_lang = [\n",
    "    'this is my book',\n",
    "    'those are your books',\n",
    "    'what is your name',\n",
    "    'i will be back',\n",
    "    'go out and about'\n",
    "]\n",
    "\n",
    "# tranform to indexed representation of sentences\n",
    "sents_indices = [[w2i.get(word, w2i['<unk>']) for word in sent.split()] for sent in sents_lang]\n",
    "\n",
    "# print representations\n",
    "print_md('#### Sentences represented as array of indices')\n",
    "print_table(['Sentence', 'Word Indices'], zip(sents_lang, sents_indices))\n",
    "# for s1, s2 in zip(sents_lang, sents_indices):\n",
    "#     print_md('*{}* => {}'.format(s1, s2))\n",
    "\n",
    "# convert sents_word_indices to a tensor\n",
    "sents_tensor = torch.tensor(sents_indices)\n",
    "print_md('#### Sentences represented as a tensor')\n",
    "print(sents_tensor)\n",
    "print('Shape = {}'.format(sents_tensor.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an embedding to lookup 10-dimensional vectors for each word\n",
    "embedding_1 = nn.Embedding(num_embeddings=len(v100), embedding_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Embedding weights"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weights = torch.Size([101, 10])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embedding of 'who'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6254, -0.9818,  0.2887,  1.6523, -0.2394,  0.1089,  0.4927,  0.7786,\n",
      "         -0.2254, -1.3948]], grad_fn=<EmbeddingBackward>)\n",
      "Shape = torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embedding of the sentence: *those are your books*"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1023,  0.3330,  1.2189,  0.1245,  1.2960, -1.4813, -1.6429, -0.5803,\n",
      "         -0.3800,  0.8051],\n",
      "        [-2.4322,  1.1259, -1.2660,  0.9573, -0.0586,  0.0960, -0.1468,  1.6027,\n",
      "          0.1917, -0.6241],\n",
      "        [ 0.5807,  1.2417,  1.2596,  1.3719,  2.0855, -0.0294, -0.4046, -2.0112,\n",
      "          0.1942,  0.5176],\n",
      "        [-2.4322,  1.1259, -1.2660,  0.9573, -0.0586,  0.0960, -0.1468,  1.6027,\n",
      "          0.1917, -0.6241]], grad_fn=<EmbeddingBackward>)\n",
      "Shape = torch.Size([4, 10])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embeddings of all 5 sentences"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.9835,  0.9457, -0.4992,  0.8422, -0.3864, -1.8317, -1.9502,\n",
      "           0.0258,  0.4130, -0.5283],\n",
      "         [-2.4322,  1.1259, -1.2660,  0.9573, -0.0586,  0.0960, -0.1468,\n",
      "           1.6027,  0.1917, -0.6241],\n",
      "         [-0.6409, -0.0850, -0.3380, -1.0989,  0.3010, -0.1985,  0.6614,\n",
      "          -0.1264, -0.6709,  3.0239],\n",
      "         [-2.4322,  1.1259, -1.2660,  0.9573, -0.0586,  0.0960, -0.1468,\n",
      "           1.6027,  0.1917, -0.6241]],\n",
      "\n",
      "        [[-1.1023,  0.3330,  1.2189,  0.1245,  1.2960, -1.4813, -1.6429,\n",
      "          -0.5803, -0.3800,  0.8051],\n",
      "         [-2.4322,  1.1259, -1.2660,  0.9573, -0.0586,  0.0960, -0.1468,\n",
      "           1.6027,  0.1917, -0.6241],\n",
      "         [ 0.5807,  1.2417,  1.2596,  1.3719,  2.0855, -0.0294, -0.4046,\n",
      "          -2.0112,  0.1942,  0.5176],\n",
      "         [-2.4322,  1.1259, -1.2660,  0.9573, -0.0586,  0.0960, -0.1468,\n",
      "           1.6027,  0.1917, -0.6241]],\n",
      "\n",
      "        [[-0.7752, -0.0060, -1.1712, -1.5884,  1.0062, -0.7441,  0.5334,\n",
      "          -0.1678,  0.5138,  0.7598],\n",
      "         [-2.4322,  1.1259, -1.2660,  0.9573, -0.0586,  0.0960, -0.1468,\n",
      "           1.6027,  0.1917, -0.6241],\n",
      "         [ 0.5807,  1.2417,  1.2596,  1.3719,  2.0855, -0.0294, -0.4046,\n",
      "          -2.0112,  0.1942,  0.5176],\n",
      "         [-2.4322,  1.1259, -1.2660,  0.9573, -0.0586,  0.0960, -0.1468,\n",
      "           1.6027,  0.1917, -0.6241]],\n",
      "\n",
      "        [[-2.4322,  1.1259, -1.2660,  0.9573, -0.0586,  0.0960, -0.1468,\n",
      "           1.6027,  0.1917, -0.6241],\n",
      "         [ 1.8664,  1.2587,  0.9737, -1.4184, -0.4242, -1.6739,  2.2437,\n",
      "           0.3722,  0.1896, -1.6523],\n",
      "         [-0.0731, -0.3132, -0.9514, -0.5488,  0.6757,  1.5997, -0.4545,\n",
      "          -0.8278, -0.0581, -0.8950],\n",
      "         [-2.4322,  1.1259, -1.2660,  0.9573, -0.0586,  0.0960, -0.1468,\n",
      "           1.6027,  0.1917, -0.6241]],\n",
      "\n",
      "        [[ 1.1307, -1.2449, -0.3337, -0.6486,  2.1481,  2.0302, -0.4627,\n",
      "          -1.1966, -0.4090, -0.0377],\n",
      "         [ 0.9686,  0.2519,  1.1117, -0.6490, -0.3481,  0.0613,  0.4885,\n",
      "          -1.3519, -1.5004,  0.5152],\n",
      "         [ 1.0264, -0.2677,  0.7468,  0.3232, -0.0832, -0.0246,  0.7653,\n",
      "           0.9689,  0.3918, -0.3219],\n",
      "         [ 0.4980, -1.3084, -1.0291, -0.1006, -0.3073, -1.6516, -0.9483,\n",
      "          -2.3833,  0.5145,  0.8958]]], grad_fn=<EmbeddingBackward>)\n",
      "Shape = torch.Size([5, 4, 10])\n"
     ]
    }
   ],
   "source": [
    "# underneath embeddings are weights, initialized with random values ~ N(0, 1)\n",
    "print_md(\"#### Embedding weights\")\n",
    "print(\"Shape of weights = {}\".format(embedding_1.weight.shape))\n",
    "\n",
    "# what's the embedding for the word 'who'\n",
    "embed = embedding_1(torch.tensor([ w2i['who']]))\n",
    "print_md(\"#### Embedding of 'who'\")\n",
    "print(\"{}\\nShape = {}\".format(embed, embed.shape))\n",
    "\n",
    "# what's the embedding of 2nd sentence\n",
    "embed = embedding_1(sents_tensor[1])\n",
    "print_md(\"#### Embedding of the sentence: *{}*\".format(sents_lang[1]))\n",
    "print(\"{}\\nShape = {}\".format(embed, embed.shape))\n",
    "\n",
    "# embeddings for all sentences\n",
    "embed = embedding_1(sents_tensor)\n",
    "print_md(\"#### Embeddings of all 5 sentences\")\n",
    "print(\"{}\\nShape = {}\".format(embed, embed.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little sophisticated embedding layer\n",
    "# Suppose the embeddings we want to lookup are to be linear transformed. This can be done by implementing a custom module wrapping nn.Embedding\n",
    "# This module applies a linear transformation on word embeddings of size 'in_embedding_dim', the output is embeddings of size 'out_embedding_dim'\n",
    "class CustomEmbedding1(nn.Module):\n",
    "    def __init__(self, num_embeddings, in_embedding_dim, out_embedding_dim):\n",
    "        super(CustomEmbedding1, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, in_embedding_dim)\n",
    "        self.linear = nn.Linear(in_features=in_embedding_dim, out_features=out_embedding_dim)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return F.relu_(self.linear(self.embedding(input)))\n",
    "\n",
    "cust_embedding_1 = CustomEmbedding1(num_embeddings=len(v100), in_embedding_dim=10, out_embedding_dim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Embedding of 'who'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0995, 0.8117, 0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward1>)\n",
      "Shape = torch.Size([1, 5])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embedding of the sentence: *those are your books*"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3331, 0.2566, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4187, 0.6632, 0.1346, 0.0000],\n",
      "        [0.0000, 0.2336, 0.0000, 0.0000, 0.9989],\n",
      "        [0.0000, 0.4187, 0.6632, 0.1346, 0.0000]], grad_fn=<ReluBackward1>)\n",
      "Shape = torch.Size([4, 5])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embeddings of all 5 sentences"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.1082, 0.0000, 0.3293, 0.0000, 0.6195],\n",
      "         [0.0000, 0.4187, 0.6632, 0.1346, 0.0000],\n",
      "         [0.6458, 1.0109, 0.0591, 0.0000, 0.3925],\n",
      "         [0.0000, 0.4187, 0.6632, 0.1346, 0.0000]],\n",
      "\n",
      "        [[0.3331, 0.2566, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.4187, 0.6632, 0.1346, 0.0000],\n",
      "         [0.0000, 0.2336, 0.0000, 0.0000, 0.9989],\n",
      "         [0.0000, 0.4187, 0.6632, 0.1346, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.4211, 0.0000, 0.0780, 0.5354],\n",
      "         [0.0000, 0.4187, 0.6632, 0.1346, 0.0000],\n",
      "         [0.0000, 0.2336, 0.0000, 0.0000, 0.9989],\n",
      "         [0.0000, 0.4187, 0.6632, 0.1346, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.4187, 0.6632, 0.1346, 0.0000],\n",
      "         [0.0000, 0.9825, 0.2333, 0.0000, 0.2550],\n",
      "         [0.0000, 0.1848, 0.3427, 0.6653, 0.3540],\n",
      "         [0.0000, 0.4187, 0.6632, 0.1346, 0.0000]],\n",
      "\n",
      "        [[0.4105, 0.0000, 0.1131, 0.0000, 0.5536],\n",
      "         [0.0000, 1.6048, 0.0000, 0.0000, 0.7310],\n",
      "         [0.2285, 0.7793, 0.0000, 0.0000, 0.2601],\n",
      "         [0.0000, 0.0000, 0.0000, 0.4725, 0.9840]]], grad_fn=<ReluBackward1>)\n",
      "Shape = torch.Size([5, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "# what's the embedding for the word 'who'\n",
    "cust_embed = cust_embedding_1(torch.tensor([ w2i['who']]))\n",
    "print_md(\"#### Embedding of 'who'\")\n",
    "print(\"{}\\nShape = {}\".format(cust_embed, cust_embed.shape))\n",
    "\n",
    "# what's the embedding of 2nd sentence\n",
    "cust_embed = cust_embedding_1(sents_tensor[1])\n",
    "print_md(\"#### Embedding of the sentence: *{}*\".format(sents_lang[1]))\n",
    "print(\"{}\\nShape = {}\".format(cust_embed, cust_embed.shape))\n",
    "\n",
    "# embeddings for all sentences\n",
    "cust_embed = cust_embedding_1(sents_tensor)\n",
    "print_md(\"#### Embeddings of all 5 sentences\")\n",
    "print(\"{}\\nShape = {}\".format(cust_embed, cust_embed.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv1d\n",
    "[API Docs](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2340, -1.8540,  0.6284],\n",
      "         [ 0.2118, -0.0350,  0.4143],\n",
      "         [-0.0295, -0.2244,  0.3394],\n",
      "         [ 0.5877,  0.4328,  0.5650]]], grad_fn=<SqueezeBackward1>)\n",
      "tensor([[[ 0.2809, -0.7985,  0.0949],\n",
      "         [-0.4016,  0.0064,  1.0939],\n",
      "         [-0.1622, -0.2143, -0.2921],\n",
      "         [-0.3205, -0.4783,  0.4445]],\n",
      "\n",
      "        [[ 0.2340, -1.8540,  0.6284],\n",
      "         [ 0.2118, -0.0350,  0.4143],\n",
      "         [-0.0295, -0.2244,  0.3394],\n",
      "         [ 0.5877,  0.4328,  0.5650]],\n",
      "\n",
      "        [[-0.0126, -1.8540,  0.6284],\n",
      "         [ 0.5771, -0.0350,  0.4143],\n",
      "         [-0.3881, -0.2244,  0.3394],\n",
      "         [-0.0738,  0.4328,  0.5650]],\n",
      "\n",
      "        [[-0.6199,  0.3852, -0.2705],\n",
      "         [-0.0599, -0.9549,  0.4671],\n",
      "         [-0.7051, -0.9074,  0.6812],\n",
      "         [ 1.1780, -0.7293, -0.3270]],\n",
      "\n",
      "        [[-0.4211,  0.2234,  0.2206],\n",
      "         [-0.4037,  0.3361, -0.2384],\n",
      "         [ 0.0705,  0.0461, -0.7122],\n",
      "         [-0.0210,  0.4416, -0.3803]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# This example illustrates usage of nn.conv1d\n",
    "\n",
    "# Imagine input is a sentence of words\n",
    "# We lookup embeddings for each word. If sentence length is 'l' and embeddings' size is 'n', then each sentence is a (l x n) matrix\n",
    "# We want to apply a 1d convolution such that there are 'm' output channels, hence there will be 'm' kernels/filters\n",
    "# if 'k' is the kernel size, then each kernel is a (k x n) matrix\n",
    "\n",
    "# we will use the setup from above, sentenses are encoded 'sents_tensor' -- a tensor of word indices\n",
    "# we now use embedding layer from above to resolve word embeddings for each word\n",
    "sents_embedded = embedding_1(sents_tensor)\n",
    "# shape of sents_embedded = (s x l x n); where s = batch_size (no.of sentences), l = length of sentence, n = embedding size\n",
    "\n",
    "batch_size, sent_len, embed_size = sents_embedded.shape\n",
    "\n",
    "out_channels = 4\n",
    "kernel_size = 2\n",
    "\n",
    "c1d = nn.Conv1d(in_channels=embed_size, out_channels=4, kernel_size=kernel_size)\n",
    "\n",
    "# let's apply convolution on 1 sentence, which is a (sent_len x embed_size) matrix\n",
    "print(c1d(sents_embedded[1].transpose(0, 1).unsqueeze(0)))\n",
    "print(c1d(sents_embedded.transpose(1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 10])\n",
      "torch.Size([5, 10, 4])\n",
      "torch.Size([10, 4])\n",
      "torch.Size([1, 10, 4])\n"
     ]
    }
   ],
   "source": [
    "print(sents_embedded.shape)\n",
    "print(sents_embedded.transpose(1, 2).shape)\n",
    "print(sents_embedded.transpose(1, 2)[0].shape)\n",
    "print(sents_embedded.transpose(1, 2)[0].unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
