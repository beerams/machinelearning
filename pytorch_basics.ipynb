{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usages of various PyTorch apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "def print_table(header, rows):\n",
    "    md = ['|' + ' | '.join(header) + '|']\n",
    "    for i, row in enumerate(rows):\n",
    "        if i == 0:\n",
    "            n_cols = len(row)\n",
    "            md.append('|--'*n_cols + '|')\n",
    "        md.append('| {} |'.format(' | '.join([str(cell) for cell in row])))\n",
    "    display_markdown('\\n'.join(md), raw=True)\n",
    "\n",
    "def print_md(s):\n",
    "    display_markdown(s, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "[API Docs](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Sentences represented as array of indices"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Sentence | Word Indices|\n",
       "|--|--|\n",
       "| this is my book | [13, 100, 88, 100] |\n",
       "| those are your books | [61, 100, 78, 100] |\n",
       "| what is your name | [11, 100, 78, 100] |\n",
       "| i will be back | [100, 33, 98, 100] |\n",
       "| go out and about | [69, 40, 70, 25] |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Sentences represented as a tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 13, 100,  88, 100],\n",
      "        [ 61, 100,  78, 100],\n",
      "        [ 11, 100,  78, 100],\n",
      "        [100,  33,  98, 100],\n",
      "        [ 69,  40,  70,  25]])\n",
      "Shape = torch.Size([5, 4])\n"
     ]
    }
   ],
   "source": [
    "# This example illustrates the usage of nn.Embedding\n",
    "# Embedding is a lookup table -- to lookup a vector stored against a key (typically an integer index)\n",
    "# You supply a bunch of keys, you get back corresponding bunch of vectors\n",
    "\n",
    "# This example tries to lookup vectors corresponding to words in a sentence\n",
    "# Setup: here are 100 english words. Index of 'how' is 0, index of 'to' is 1, ... index of 'two' is 99\n",
    "v100 = ['how', 'to', 'her', 'at', 'up', 'see', 'in', 'thing', 'even', 'because', 'or', \n",
    "             'what', 'man', 'this', 'for', 'with', 'time', 'now', 'give', 'very', 'take', 'other', \n",
    "             'there', 'would', 'first', 'about', 'people', 'think', 'find', 'so', 'say', 'as', \n",
    "             'many', 'will', 'just', 'he', 'I', 'well', 'our', 'tell', 'out', 'have', 'can', 'its', \n",
    "             'make', 'get', 'if', 'than', 'use', 'that', 'new', 'also', 'from', 'by', 'his', 'year', \n",
    "             'do', 'some', 'the', 'no', 'a', 'those', 'she', 'come', 'one', 'their', 'more', \n",
    "             'these', 'all', 'go', 'and', 'could', 'him', 'into', 'only', 'who', 'of', 'it', 'your', \n",
    "             'not', 'you', 'here', 'when', 'on', 'which', 'then', 'know', 'them', 'my', 'me', 'we', \n",
    "             'want', 'they', 'like', 'look', 'day', 'way', 'but', 'be', 'two']\n",
    "\n",
    "# Naturally this vocabulary is insufficient even for most common sentences. Map all out of vocabulary words to <unk>\n",
    "# add <unk> to vocabulary at index 100. <unk> is the replacement for out of vocabulary words\n",
    "v100.append('<unk>')\n",
    "\n",
    "# create a dictionary to lookup word's index\n",
    "w2i = {word: i for i, word in enumerate(v100)}\n",
    "\n",
    "# define few sentences of fixed size. we wish to lookup embeddings for words in these sentences\n",
    "# here are 5 sentences\n",
    "sents_lang = [\n",
    "    'this is my book',\n",
    "    'those are your books',\n",
    "    'what is your name',\n",
    "    'i will be back',\n",
    "    'go out and about'\n",
    "]\n",
    "\n",
    "# tranform to indexed representation of sentences\n",
    "sents_indices = [[w2i.get(word, w2i['<unk>']) for word in sent.split()] for sent in sents_lang]\n",
    "\n",
    "# print representations\n",
    "print_md('#### Sentences represented as array of indices')\n",
    "print_table(['Sentence', 'Word Indices'], zip(sents_lang, sents_indices))\n",
    "# for s1, s2 in zip(sents_lang, sents_indices):\n",
    "#     print_md('*{}* => {}'.format(s1, s2))\n",
    "\n",
    "# convert sents_word_indices to a tensor\n",
    "sents_tensor = torch.tensor(sents_indices)\n",
    "print_md('#### Sentences represented as a tensor')\n",
    "print(sents_tensor)\n",
    "print('Shape = {}'.format(sents_tensor.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an embedding to lookup 10-dimensional vectors for each word\n",
    "embedding_1 = nn.Embedding(num_embeddings=len(v100), embedding_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Embedding weights"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weights = torch.Size([101, 10])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embedding of 'who'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2825, -1.0863, -1.0729, -0.3079, -2.3465, -0.4001,  0.0555,  1.8038,\n",
      "         -0.4832, -2.5920]], grad_fn=<EmbeddingBackward>)\n",
      "Shape = torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embedding of the sentence: *those are your books*"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7780,  0.9255,  0.4747,  0.2262,  1.1915,  0.3400, -0.2099, -0.2856,\n",
      "         -0.2899,  0.6223],\n",
      "        [ 2.3802, -0.6466, -0.5523,  1.0575,  0.7071,  0.0096, -0.5120,  0.3442,\n",
      "          0.0671,  2.0389],\n",
      "        [ 0.6997,  0.7869, -1.2694, -0.2669,  0.7621, -1.1303,  0.1839, -0.8978,\n",
      "         -1.0707,  0.3894],\n",
      "        [ 2.3802, -0.6466, -0.5523,  1.0575,  0.7071,  0.0096, -0.5120,  0.3442,\n",
      "          0.0671,  2.0389]], grad_fn=<EmbeddingBackward>)\n",
      "Shape = torch.Size([4, 10])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embeddings of all 5 sentences"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 8.7368e-01,  9.1121e-01, -1.4728e+00, -1.1883e-01,  1.3988e-01,\n",
      "           4.7449e-01, -2.4134e-01,  2.0292e-01,  8.0251e-03,  1.5162e-01],\n",
      "         [ 2.3802e+00, -6.4662e-01, -5.5234e-01,  1.0575e+00,  7.0708e-01,\n",
      "           9.6448e-03, -5.1204e-01,  3.4416e-01,  6.7109e-02,  2.0389e+00],\n",
      "         [ 1.3637e+00,  9.8815e-01,  3.6328e-01, -1.7120e-01, -1.4109e+00,\n",
      "          -2.0556e+00,  6.2491e-01,  1.6375e+00, -3.4429e-01,  1.3861e+00],\n",
      "         [ 2.3802e+00, -6.4662e-01, -5.5234e-01,  1.0575e+00,  7.0708e-01,\n",
      "           9.6448e-03, -5.1204e-01,  3.4416e-01,  6.7109e-02,  2.0389e+00]],\n",
      "\n",
      "        [[ 7.7800e-01,  9.2552e-01,  4.7475e-01,  2.2625e-01,  1.1915e+00,\n",
      "           3.4001e-01, -2.0990e-01, -2.8558e-01, -2.8991e-01,  6.2229e-01],\n",
      "         [ 2.3802e+00, -6.4662e-01, -5.5234e-01,  1.0575e+00,  7.0708e-01,\n",
      "           9.6448e-03, -5.1204e-01,  3.4416e-01,  6.7109e-02,  2.0389e+00],\n",
      "         [ 6.9966e-01,  7.8688e-01, -1.2694e+00, -2.6687e-01,  7.6214e-01,\n",
      "          -1.1303e+00,  1.8390e-01, -8.9775e-01, -1.0707e+00,  3.8938e-01],\n",
      "         [ 2.3802e+00, -6.4662e-01, -5.5234e-01,  1.0575e+00,  7.0708e-01,\n",
      "           9.6448e-03, -5.1204e-01,  3.4416e-01,  6.7109e-02,  2.0389e+00]],\n",
      "\n",
      "        [[ 4.7311e-01,  3.7745e-01, -8.6564e-02,  9.5387e-01, -2.1195e-01,\n",
      "          -7.4174e-02,  9.2922e-02, -1.9114e-01, -4.2647e-01,  5.0507e-01],\n",
      "         [ 2.3802e+00, -6.4662e-01, -5.5234e-01,  1.0575e+00,  7.0708e-01,\n",
      "           9.6448e-03, -5.1204e-01,  3.4416e-01,  6.7109e-02,  2.0389e+00],\n",
      "         [ 6.9966e-01,  7.8688e-01, -1.2694e+00, -2.6687e-01,  7.6214e-01,\n",
      "          -1.1303e+00,  1.8390e-01, -8.9775e-01, -1.0707e+00,  3.8938e-01],\n",
      "         [ 2.3802e+00, -6.4662e-01, -5.5234e-01,  1.0575e+00,  7.0708e-01,\n",
      "           9.6448e-03, -5.1204e-01,  3.4416e-01,  6.7109e-02,  2.0389e+00]],\n",
      "\n",
      "        [[ 2.3802e+00, -6.4662e-01, -5.5234e-01,  1.0575e+00,  7.0708e-01,\n",
      "           9.6448e-03, -5.1204e-01,  3.4416e-01,  6.7109e-02,  2.0389e+00],\n",
      "         [-1.0856e+00, -1.3119e+00, -8.1797e-01, -2.3838e-01, -7.5246e-01,\n",
      "           7.9488e-01, -3.4586e-02,  2.2512e+00,  1.3933e+00, -4.8261e-01],\n",
      "         [ 5.7156e-01,  1.3556e+00, -2.6613e-01, -6.0271e-01,  3.7473e-01,\n",
      "          -8.7185e-01,  1.4856e-01, -2.2998e+00, -1.5971e+00, -2.1424e+00],\n",
      "         [ 2.3802e+00, -6.4662e-01, -5.5234e-01,  1.0575e+00,  7.0708e-01,\n",
      "           9.6448e-03, -5.1204e-01,  3.4416e-01,  6.7109e-02,  2.0389e+00]],\n",
      "\n",
      "        [[-2.1420e-01, -1.0366e+00, -1.5573e-02, -3.8114e-01, -1.1037e-01,\n",
      "          -3.1308e-01, -1.3987e+00,  1.7597e-01, -1.4538e+00,  3.6518e-01],\n",
      "         [-5.8024e-01, -3.8951e-02,  9.0804e-01,  6.0873e-01,  2.4324e+00,\n",
      "          -1.4015e+00, -7.0389e-01,  1.7464e+00, -1.5945e+00,  8.2188e-01],\n",
      "         [-5.4427e-01, -1.2207e-01,  2.7288e-01,  2.4749e-01,  1.4777e+00,\n",
      "           1.3007e-01,  4.6856e-01, -5.4410e-01, -7.4037e-01,  6.5440e-01],\n",
      "         [ 9.8307e-01, -2.6393e-04, -9.4379e-01,  1.8835e-01,  1.1757e+00,\n",
      "          -1.4151e+00,  1.3478e+00,  2.4648e-01,  8.1470e-01,  2.0409e+00]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "Shape = torch.Size([5, 4, 10])\n"
     ]
    }
   ],
   "source": [
    "# underneath embeddings are weights, initialized with random values ~ N(0, 1)\n",
    "print_md(\"#### Embedding weights\")\n",
    "print(\"Shape of weights = {}\".format(embedding_1.weight.shape))\n",
    "\n",
    "# what's the embedding for the word 'who'\n",
    "embed = embedding_1(torch.tensor([ w2i['who']]))\n",
    "print_md(\"#### Embedding of 'who'\")\n",
    "print(\"{}\\nShape = {}\".format(embed, embed.shape))\n",
    "\n",
    "# what's the embedding of 2nd sentence\n",
    "embed = embedding_1(sents_tensor[1])\n",
    "print_md(\"#### Embedding of the sentence: *{}*\".format(sents_lang[1]))\n",
    "print(\"{}\\nShape = {}\".format(embed, embed.shape))\n",
    "\n",
    "# embeddings for all sentences\n",
    "embed = embedding_1(sents_tensor)\n",
    "print_md(\"#### Embeddings of all 5 sentences\")\n",
    "print(\"{}\\nShape = {}\".format(embed, embed.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little sophisticated embedding layer\n",
    "# Suppose the embeddings we want to lookup are to be linear transformed. This can be done by implementing a custom module wrapping nn.Embedding\n",
    "# This module applies a linear transformation on word embeddings of size 'in_embedding_dim', the output is embeddings of size 'out_embedding_dim'\n",
    "class CustomEmbedding1(nn.Module):\n",
    "    def __init__(self, num_embeddings, in_embedding_dim, out_embedding_dim):\n",
    "        super(CustomEmbedding1, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, in_embedding_dim)\n",
    "        self.linear = nn.Linear(in_features=in_embedding_dim, out_features=out_embedding_dim)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return F.relu_(self.linear(self.embedding(input)))\n",
    "\n",
    "cust_embedding_1 = CustomEmbedding1(num_embeddings=len(v100), in_embedding_dim=10, out_embedding_dim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Embedding of 'who'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7801, 0.4595, 0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward1>)\n",
      "Shape = torch.Size([1, 5])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embedding of the sentence: *those are your books*"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 1.8667, 0.7490, 1.1981, 0.4445],\n",
      "        [0.4229, 0.2298, 0.0000, 0.0000, 0.5433],\n",
      "        [0.5156, 0.7353, 0.6418, 0.6842, 0.6846],\n",
      "        [0.4229, 0.2298, 0.0000, 0.0000, 0.5433]], grad_fn=<ReluBackward1>)\n",
      "Shape = torch.Size([4, 5])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embeddings of all 5 sentences"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.0157, 0.0769, 0.0000, 0.0000],\n",
      "         [0.4229, 0.2298, 0.0000, 0.0000, 0.5433],\n",
      "         [0.0000, 0.9904, 0.9481, 0.5331, 0.0000],\n",
      "         [0.4229, 0.2298, 0.0000, 0.0000, 0.5433]],\n",
      "\n",
      "        [[0.0000, 1.8667, 0.7490, 1.1981, 0.4445],\n",
      "         [0.4229, 0.2298, 0.0000, 0.0000, 0.5433],\n",
      "         [0.5156, 0.7353, 0.6418, 0.6842, 0.6846],\n",
      "         [0.4229, 0.2298, 0.0000, 0.0000, 0.5433]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4229, 0.2298, 0.0000, 0.0000, 0.5433],\n",
      "         [0.5156, 0.7353, 0.6418, 0.6842, 0.6846],\n",
      "         [0.4229, 0.2298, 0.0000, 0.0000, 0.5433]],\n",
      "\n",
      "        [[0.4229, 0.2298, 0.0000, 0.0000, 0.5433],\n",
      "         [0.2141, 0.0000, 1.1520, 0.9001, 0.0000],\n",
      "         [0.0000, 0.2403, 0.0000, 0.0072, 0.0000],\n",
      "         [0.4229, 0.2298, 0.0000, 0.0000, 0.5433]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.6113, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 1.4576, 0.9578, 0.3138]]], grad_fn=<ReluBackward1>)\n",
      "Shape = torch.Size([5, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "# what's the embedding for the word 'who'\n",
    "cust_embed = cust_embedding_1(torch.tensor([ w2i['who']]))\n",
    "print_md(\"#### Embedding of 'who'\")\n",
    "print(\"{}\\nShape = {}\".format(cust_embed, cust_embed.shape))\n",
    "\n",
    "# what's the embedding of 2nd sentence\n",
    "cust_embed = cust_embedding_1(sents_tensor[1])\n",
    "print_md(\"#### Embedding of the sentence: *{}*\".format(sents_lang[1]))\n",
    "print(\"{}\\nShape = {}\".format(cust_embed, cust_embed.shape))\n",
    "\n",
    "# embeddings for all sentences\n",
    "cust_embed = cust_embedding_1(sents_tensor)\n",
    "print_md(\"#### Embeddings of all 5 sentences\")\n",
    "print(\"{}\\nShape = {}\".format(cust_embed, cust_embed.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv1d\n",
    "[API Docs](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example illustrates usage of nn.conv1d\n",
    "\n",
    "# Imagine input is a sentence of words\n",
    "# We lookup embeddings for each word. If sentence length is 'l' and embeddings' size is 'n', then each sentence is a (l x n) matrix\n",
    "# We want to apply a 1d convolution such that there are 'm' output channels, hence there will be 'm' kernels/filters\n",
    "# if 'k' is the kernel size, then each kernel is a (k x n) matrix\n",
    "\n",
    "# we will use the setup from above, sentenses are encoded 'sents_tensor' -- a tensor of word indices\n",
    "# we now use embedding layer from above to resolve word embeddings for each word\n",
    "sents_embedded = embedding_1(sents_tensor)\n",
    "# shape of sents_embedded = (s x l x n); where s = batch_size (no.of sentences), l = length of sentence, n = embedding size\n",
    "\n",
    "batch_size, sent_len, embed_size = sents_embedded.shape\n",
    "\n",
    "out_channels = 4\n",
    "kernel_size = 2\n",
    "\n",
    "c1d = nn.Conv1d(in_channels=embed_size, out_channels=4, kernel_size=kernel_size)\n",
    "\n",
    "# let's apply convolution on 1 sentence, which is a (sent_len x embed_size) matrix\n",
    "print(c1d(sents_embedded[1].transpose(0, 1).unsqueeze(0)))\n",
    "print(c1d(sents_embedded.transpose(1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sents_embedded.shape)\n",
    "print(sents_embedded.transpose(1, 2).shape)\n",
    "print(sents_embedded.transpose(1, 2)[0].shape)\n",
    "print(sents_embedded.transpose(1, 2)[0].unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
