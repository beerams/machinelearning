{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usages of various PyTorch apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ipython_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Sentences represented as array of indices"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Sentence | Word Indices|\n",
       "|--|--|\n",
       "| this is my book | [13, 100, 88, 100] |\n",
       "| those are your books | [61, 100, 78, 100] |\n",
       "| what is your name | [11, 100, 78, 100] |\n",
       "| i will be back | [100, 33, 98, 100] |\n",
       "| go out and about | [69, 40, 70, 25] |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Sentences represented as a tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 13, 100,  88, 100],\n",
      "        [ 61, 100,  78, 100],\n",
      "        [ 11, 100,  78, 100],\n",
      "        [100,  33,  98, 100],\n",
      "        [ 69,  40,  70,  25]])\n",
      "Shape = torch.Size([5, 4])\n"
     ]
    }
   ],
   "source": [
    "# The data setup here is used to illustrate usage of various apis\n",
    "# here are 100 english words. Index of 'how' is 0, index of 'to' is 1, ... index of 'two' is 99\n",
    "v100 = ['how', 'to', 'her', 'at', 'up', 'see', 'in', 'thing', 'even', 'because', 'or', \n",
    "             'what', 'man', 'this', 'for', 'with', 'time', 'now', 'give', 'very', 'take', 'other', \n",
    "             'there', 'would', 'first', 'about', 'people', 'think', 'find', 'so', 'say', 'as', \n",
    "             'many', 'will', 'just', 'he', 'I', 'well', 'our', 'tell', 'out', 'have', 'can', 'its', \n",
    "             'make', 'get', 'if', 'than', 'use', 'that', 'new', 'also', 'from', 'by', 'his', 'year', \n",
    "             'do', 'some', 'the', 'no', 'a', 'those', 'she', 'come', 'one', 'their', 'more', \n",
    "             'these', 'all', 'go', 'and', 'could', 'him', 'into', 'only', 'who', 'of', 'it', 'your', \n",
    "             'not', 'you', 'here', 'when', 'on', 'which', 'then', 'know', 'them', 'my', 'me', 'we', \n",
    "             'want', 'they', 'like', 'look', 'day', 'way', 'but', 'be', 'two']\n",
    "\n",
    "# Naturally this vocabulary is insufficient even for most common sentences. So we map all out of vocabulary words to <unk>\n",
    "# add <unk> to vocabulary at index 100. <unk> is the replacement for out of vocabulary words\n",
    "v100.append('<unk>')\n",
    "\n",
    "# create a dictionary to lookup word's index\n",
    "w2i = {word: i for i, word in enumerate(v100)}\n",
    "\n",
    "# define few sentences of fixed size. we wish to lookup embeddings for words in these sentences\n",
    "# here are 5 sentences\n",
    "sents_lang = [\n",
    "    'this is my book',\n",
    "    'those are your books',\n",
    "    'what is your name',\n",
    "    'i will be back',\n",
    "    'go out and about'\n",
    "]\n",
    "\n",
    "# tranform to indexed representation of sentences\n",
    "sents_indices = [[w2i.get(word, w2i['<unk>']) for word in sent.split()] for sent in sents_lang]\n",
    "\n",
    "# print representations\n",
    "print_h4('Sentences represented as array of indices')\n",
    "print_table(['Sentence', 'Word Indices'], zip(sents_lang, sents_indices))\n",
    "\n",
    "# convert sents_word_indices to a tensor\n",
    "sents_tensor = torch.tensor(sents_indices)\n",
    "print_h4('Sentences represented as a tensor')\n",
    "print(sents_tensor)\n",
    "print('Shape = {}'.format(sents_tensor.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "[API Docs](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example illustrates the usage of nn.Embedding\n",
    "# Embedding is a lookup table -- to lookup a vector stored against a key (typically an integer index)\n",
    "# You supply a bunch of keys, you get back corresponding bunch of vectors\n",
    "\n",
    "# create an embedding to lookup 10-dimensional vectors for each word\n",
    "embedding_1 = nn.Embedding(num_embeddings=len(v100), embedding_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Embedding weights"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weights = torch.Size([101, 10])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embedding of 'who'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5085,  2.0812,  0.2063, -0.0750,  0.1841, -0.4679, -1.4961, -1.6274,\n",
      "          1.3843, -1.5742]], grad_fn=<EmbeddingBackward>)\n",
      "Shape = torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embedding of the sentence: *those are your books*"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0256, -1.6001, -1.0172,  0.0850, -0.3718,  1.2429, -1.1077, -2.6282,\n",
      "         -0.5212,  0.4638],\n",
      "        [-0.3261, -1.0868, -0.4142,  0.9241,  0.7116,  0.8363, -0.3185, -0.3769,\n",
      "         -0.3824, -1.1299],\n",
      "        [-0.1356, -0.8241,  0.5238,  0.0998,  0.4279, -0.0106, -0.1339, -0.5938,\n",
      "         -0.7397,  1.6691],\n",
      "        [-0.3261, -1.0868, -0.4142,  0.9241,  0.7116,  0.8363, -0.3185, -0.3769,\n",
      "         -0.3824, -1.1299]], grad_fn=<EmbeddingBackward>)\n",
      "Shape = torch.Size([4, 10])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embeddings of all 5 sentences"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0786, -0.4692, -1.5821,  0.8976, -2.1452,  0.9335,  0.1841,\n",
      "          -0.3110, -0.8167, -0.5015],\n",
      "         [-0.3261, -1.0868, -0.4142,  0.9241,  0.7116,  0.8363, -0.3185,\n",
      "          -0.3769, -0.3824, -1.1299],\n",
      "         [ 3.2290,  1.5537, -0.3121, -0.7413, -0.2396, -0.3453,  1.2686,\n",
      "          -1.2152, -0.0532,  0.1395],\n",
      "         [-0.3261, -1.0868, -0.4142,  0.9241,  0.7116,  0.8363, -0.3185,\n",
      "          -0.3769, -0.3824, -1.1299]],\n",
      "\n",
      "        [[-0.0256, -1.6001, -1.0172,  0.0850, -0.3718,  1.2429, -1.1077,\n",
      "          -2.6282, -0.5212,  0.4638],\n",
      "         [-0.3261, -1.0868, -0.4142,  0.9241,  0.7116,  0.8363, -0.3185,\n",
      "          -0.3769, -0.3824, -1.1299],\n",
      "         [-0.1356, -0.8241,  0.5238,  0.0998,  0.4279, -0.0106, -0.1339,\n",
      "          -0.5938, -0.7397,  1.6691],\n",
      "         [-0.3261, -1.0868, -0.4142,  0.9241,  0.7116,  0.8363, -0.3185,\n",
      "          -0.3769, -0.3824, -1.1299]],\n",
      "\n",
      "        [[ 2.9985, -0.4593, -0.1563, -1.3368,  0.3185,  0.1849, -0.8071,\n",
      "           1.8832,  0.2526, -0.1477],\n",
      "         [-0.3261, -1.0868, -0.4142,  0.9241,  0.7116,  0.8363, -0.3185,\n",
      "          -0.3769, -0.3824, -1.1299],\n",
      "         [-0.1356, -0.8241,  0.5238,  0.0998,  0.4279, -0.0106, -0.1339,\n",
      "          -0.5938, -0.7397,  1.6691],\n",
      "         [-0.3261, -1.0868, -0.4142,  0.9241,  0.7116,  0.8363, -0.3185,\n",
      "          -0.3769, -0.3824, -1.1299]],\n",
      "\n",
      "        [[-0.3261, -1.0868, -0.4142,  0.9241,  0.7116,  0.8363, -0.3185,\n",
      "          -0.3769, -0.3824, -1.1299],\n",
      "         [ 0.6631,  0.5790, -1.3194,  0.5884, -0.8221,  0.5095,  1.2186,\n",
      "           0.1207,  0.2631, -0.3368],\n",
      "         [ 1.3343,  1.0167, -2.0121,  1.2629,  1.3422,  0.0469, -0.7305,\n",
      "          -0.2865,  1.0202, -1.6767],\n",
      "         [-0.3261, -1.0868, -0.4142,  0.9241,  0.7116,  0.8363, -0.3185,\n",
      "          -0.3769, -0.3824, -1.1299]],\n",
      "\n",
      "        [[-0.6165, -0.6526,  0.5102,  1.7093,  0.7737, -0.8334, -0.8577,\n",
      "          -0.9922, -1.7238,  0.6800],\n",
      "         [-2.1714, -2.6847,  0.3110, -1.0095, -1.2619,  1.5996,  2.2719,\n",
      "           0.9253,  0.0352, -1.2095],\n",
      "         [-0.6596, -0.5567,  0.2573,  0.3597, -1.0896,  0.4419,  2.1832,\n",
      "           1.7973, -1.2979,  0.2915],\n",
      "         [ 0.5067,  0.6742,  1.1739, -0.1535,  0.1058,  0.8562, -0.3416,\n",
      "          -0.5625,  0.6085, -0.0737]]], grad_fn=<EmbeddingBackward>)\n",
      "Shape = torch.Size([5, 4, 10])\n"
     ]
    }
   ],
   "source": [
    "# underneath embeddings are weights, initialized with random values ~ N(0, 1)\n",
    "print_md(\"#### Embedding weights\")\n",
    "print(\"Shape of weights = {}\".format(embedding_1.weight.shape))\n",
    "\n",
    "# what's the embedding for the word 'who'\n",
    "embed = embedding_1(torch.tensor([ w2i['who']]))\n",
    "print_h4(\"Embedding of 'who'\")\n",
    "print(\"{}\\nShape = {}\".format(embed, embed.shape))\n",
    "\n",
    "# what's the embedding of 2nd sentence\n",
    "embed = embedding_1(sents_tensor[1])\n",
    "print_h4(\"Embedding of the sentence: *{}*\".format(sents_lang[1]))\n",
    "print(\"{}\\nShape = {}\".format(embed, embed.shape))\n",
    "\n",
    "# embeddings for all sentences\n",
    "embed = embedding_1(sents_tensor)\n",
    "print_h4(\"Embeddings of all 5 sentences\")\n",
    "print(\"{}\\nShape = {}\".format(embed, embed.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little sophisticated embedding layer\n",
    "# Suppose the embeddings we want to lookup are to be linear transformed. This can be done by implementing a custom module wrapping nn.Embedding\n",
    "# This module applies a linear transformation on word embeddings of size 'in_embedding_dim', the output is embeddings of size 'out_embedding_dim'\n",
    "class CustomEmbedding1(nn.Module):\n",
    "    def __init__(self, num_embeddings, in_embedding_dim, out_embedding_dim):\n",
    "        super(CustomEmbedding1, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, in_embedding_dim)\n",
    "        self.linear = nn.Linear(in_features=in_embedding_dim, out_features=out_embedding_dim)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return F.relu_(self.linear(self.embedding(input)))\n",
    "\n",
    "cust_embedding_1 = CustomEmbedding1(num_embeddings=len(v100), in_embedding_dim=10, out_embedding_dim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Embedding of 'who'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2956, 0.0000, 0.9272, 0.6708, 0.6879]], grad_fn=<ReluBackward1>)\n",
      "Shape = torch.Size([1, 5])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embedding of the sentence: '*those are your books*'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.5426, 0.3918, 0.8545, 0.3795],\n",
      "        [0.0000, 0.0000, 0.4898, 0.5537, 0.0000],\n",
      "        [1.0913, 0.1307, 0.4870, 0.2974, 0.4698],\n",
      "        [0.0000, 0.0000, 0.4898, 0.5537, 0.0000]], grad_fn=<ReluBackward1>)\n",
      "Shape = torch.Size([4, 5])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embeddings of all 5 sentences"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.0000, 0.6743, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.4898, 0.5537, 0.0000],\n",
      "         [0.2232, 0.4456, 0.1584, 0.5502, 0.4410],\n",
      "         [0.0000, 0.0000, 0.4898, 0.5537, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.5426, 0.3918, 0.8545, 0.3795],\n",
      "         [0.0000, 0.0000, 0.4898, 0.5537, 0.0000],\n",
      "         [1.0913, 0.1307, 0.4870, 0.2974, 0.4698],\n",
      "         [0.0000, 0.0000, 0.4898, 0.5537, 0.0000]],\n",
      "\n",
      "        [[0.2580, 0.6908, 0.5103, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.4898, 0.5537, 0.0000],\n",
      "         [1.0913, 0.1307, 0.4870, 0.2974, 0.4698],\n",
      "         [0.0000, 0.0000, 0.4898, 0.5537, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.4898, 0.5537, 0.0000],\n",
      "         [0.0000, 0.0000, 0.1952, 0.0534, 0.0000],\n",
      "         [0.0000, 0.0000, 0.1602, 0.0000, 0.1538],\n",
      "         [0.0000, 0.0000, 0.4898, 0.5537, 0.0000]],\n",
      "\n",
      "        [[0.3474, 0.1658, 0.0000, 0.4990, 0.0000],\n",
      "         [0.3664, 0.0000, 0.0000, 0.9406, 0.0000],\n",
      "         [0.5046, 0.0000, 0.4269, 1.2592, 0.0000],\n",
      "         [0.3854, 0.0000, 0.5261, 0.6353, 0.0000]]], grad_fn=<ReluBackward1>)\n",
      "Shape = torch.Size([5, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "# what's the embedding for the word 'who'\n",
    "cust_embed = cust_embedding_1(torch.tensor([ w2i['who']]))\n",
    "print_h4(\"Embedding of 'who'\")\n",
    "print(\"{}\\nShape = {}\".format(cust_embed, cust_embed.shape))\n",
    "\n",
    "# what's the embedding of 2nd sentence\n",
    "cust_embed = cust_embedding_1(sents_tensor[1])\n",
    "print_h4(\"Embedding of the sentence: '*{}*'\".format(sents_lang[1]))\n",
    "print(\"{}\\nShape = {}\".format(cust_embed, cust_embed.shape))\n",
    "\n",
    "# embeddings for all sentences\n",
    "cust_embed = cust_embedding_1(sents_tensor)\n",
    "print_h4(\"Embeddings of all 5 sentences\")\n",
    "print(\"{}\\nShape = {}\".format(cust_embed, cust_embed.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv1d\n",
    "[API Docs](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This example illustrates usage of nn.Conv1d\n",
    "\n",
    "Imagine input is a sentence of words, and we lookup embeddings for each word. \n",
    "If sentence length is 'l' and embeddings' size is 'n', then each sentence is a (l x n) matrix\n",
    "\n",
    "To apply a 1-D convolution such that there are 'm' output channels, we will need 'm' kernels/filters.\n",
    "If 'k' is the kernel size, then each kernel is a (k x n) matrix\n",
    "\n",
    "We will use the setup from above, where 'sents_tensor' is a tensor of word indices.\n",
    "\"\"\"\n",
    "\n",
    "# We first get embedded representation of sentences by applying embedding layer.\n",
    "# shape of sents_embedded = (s x l x n); where s = batch_size (no.of sentences), l = length of sentence, n = embedding size\n",
    "sents_embedded = embedding_1(sents_tensor)\n",
    "\n",
    "batch_size, sent_len, embed_size = sents_embedded.shape\n",
    "\n",
    "# no.of output channels & kernerl size are hyperparameters, for simplicity we set 4, 2 respectively\n",
    "out_channels = 4\n",
    "kernel_size = 2  \n",
    "\n",
    "# define a convolution layer\n",
    "c1d = nn.Conv1d(in_channels=embed_size, out_channels=out_channels, kernel_size=kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Convolving single sentence '*what is your name*'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded representation = tensor([[ 2.9985, -0.4593, -0.1563, -1.3368,  0.3185,  0.1849, -0.8071,  1.8832,\n",
      "          0.2526, -0.1477],\n",
      "        [-0.3261, -1.0868, -0.4142,  0.9241,  0.7116,  0.8363, -0.3185, -0.3769,\n",
      "         -0.3824, -1.1299],\n",
      "        [-0.1356, -0.8241,  0.5238,  0.0998,  0.4279, -0.0106, -0.1339, -0.5938,\n",
      "         -0.7397,  1.6691],\n",
      "        [-0.3261, -1.0868, -0.4142,  0.9241,  0.7116,  0.8363, -0.3185, -0.3769,\n",
      "         -0.3824, -1.1299]], grad_fn=<SelectBackward>)\n",
      "Shape = torch.Size([4, 10])\n",
      "\n",
      "Batched-input representation = tensor([[[ 2.9985, -0.4593, -0.1563, -1.3368,  0.3185,  0.1849, -0.8071,\n",
      "           1.8832,  0.2526, -0.1477],\n",
      "         [-0.3261, -1.0868, -0.4142,  0.9241,  0.7116,  0.8363, -0.3185,\n",
      "          -0.3769, -0.3824, -1.1299],\n",
      "         [-0.1356, -0.8241,  0.5238,  0.0998,  0.4279, -0.0106, -0.1339,\n",
      "          -0.5938, -0.7397,  1.6691],\n",
      "         [-0.3261, -1.0868, -0.4142,  0.9241,  0.7116,  0.8363, -0.3185,\n",
      "          -0.3769, -0.3824, -1.1299]]], grad_fn=<UnsqueezeBackward0>)\n",
      "Shape = torch.Size([1, 4, 10])\n",
      "\n",
      "Batched-input transposed = tensor([[[ 2.9985, -0.3261, -0.1356, -0.3261],\n",
      "         [-0.4593, -1.0868, -0.8241, -1.0868],\n",
      "         [-0.1563, -0.4142,  0.5238, -0.4142],\n",
      "         [-1.3368,  0.9241,  0.0998,  0.9241],\n",
      "         [ 0.3185,  0.7116,  0.4279,  0.7116],\n",
      "         [ 0.1849,  0.8363, -0.0106,  0.8363],\n",
      "         [-0.8071, -0.3185, -0.1339, -0.3185],\n",
      "         [ 1.8832, -0.3769, -0.5938, -0.3769],\n",
      "         [ 0.2526, -0.3824, -0.7397, -0.3824],\n",
      "         [-0.1477, -1.1299,  1.6691, -1.1299]]], grad_fn=<TransposeBackward0>)\n",
      "Shape = torch.Size([1, 10, 4])\n",
      "\n",
      "Convolution output = tensor([[[-0.1830, -0.4099, -0.3970],\n",
      "         [-1.2907,  0.2795, -0.0898],\n",
      "         [-0.3762,  0.8299,  0.8173],\n",
      "         [ 0.9007,  0.5589, -0.0469]]], grad_fn=<SqueezeBackward1>)\n",
      "Shape = torch.Size([1, 4, 3])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Convolving 2 sentences '*those are your books*' and '*i will be back*'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched-Input = tensor([[[-0.0256, -1.6001, -1.0172,  0.0850, -0.3718,  1.2429, -1.1077,\n",
      "          -2.6282, -0.5212,  0.4638],\n",
      "         [-0.3261, -1.0868, -0.4142,  0.9241,  0.7116,  0.8363, -0.3185,\n",
      "          -0.3769, -0.3824, -1.1299],\n",
      "         [-0.1356, -0.8241,  0.5238,  0.0998,  0.4279, -0.0106, -0.1339,\n",
      "          -0.5938, -0.7397,  1.6691],\n",
      "         [-0.3261, -1.0868, -0.4142,  0.9241,  0.7116,  0.8363, -0.3185,\n",
      "          -0.3769, -0.3824, -1.1299]],\n",
      "\n",
      "        [[-0.3261, -1.0868, -0.4142,  0.9241,  0.7116,  0.8363, -0.3185,\n",
      "          -0.3769, -0.3824, -1.1299],\n",
      "         [ 0.6631,  0.5790, -1.3194,  0.5884, -0.8221,  0.5095,  1.2186,\n",
      "           0.1207,  0.2631, -0.3368],\n",
      "         [ 1.3343,  1.0167, -2.0121,  1.2629,  1.3422,  0.0469, -0.7305,\n",
      "          -0.2865,  1.0202, -1.6767],\n",
      "         [-0.3261, -1.0868, -0.4142,  0.9241,  0.7116,  0.8363, -0.3185,\n",
      "          -0.3769, -0.3824, -1.1299]]], grad_fn=<IndexBackward>)\n",
      "Shape = torch.Size([2, 4, 10])\n",
      "\n",
      "Batched-input transposed = tensor([[[-0.0256, -0.3261, -0.1356, -0.3261],\n",
      "         [-1.6001, -1.0868, -0.8241, -1.0868],\n",
      "         [-1.0172, -0.4142,  0.5238, -0.4142],\n",
      "         [ 0.0850,  0.9241,  0.0998,  0.9241],\n",
      "         [-0.3718,  0.7116,  0.4279,  0.7116],\n",
      "         [ 1.2429,  0.8363, -0.0106,  0.8363],\n",
      "         [-1.1077, -0.3185, -0.1339, -0.3185],\n",
      "         [-2.6282, -0.3769, -0.5938, -0.3769],\n",
      "         [-0.5212, -0.3824, -0.7397, -0.3824],\n",
      "         [ 0.4638, -1.1299,  1.6691, -1.1299]],\n",
      "\n",
      "        [[-0.3261,  0.6631,  1.3343, -0.3261],\n",
      "         [-1.0868,  0.5790,  1.0167, -1.0868],\n",
      "         [-0.4142, -1.3194, -2.0121, -0.4142],\n",
      "         [ 0.9241,  0.5884,  1.2629,  0.9241],\n",
      "         [ 0.7116, -0.8221,  1.3422,  0.7116],\n",
      "         [ 0.8363,  0.5095,  0.0469,  0.8363],\n",
      "         [-0.3185,  1.2186, -0.7305, -0.3185],\n",
      "         [-0.3769,  0.1207, -0.2865, -0.3769],\n",
      "         [-0.3824,  0.2631,  1.0202, -0.3824],\n",
      "         [-1.1299, -0.3368, -1.6767, -1.1299]]], grad_fn=<TransposeBackward0>)\n",
      "Shape = torch.Size([2, 10, 4])\n",
      "\n",
      "Convolution output = tensor([[[-0.9116, -0.4099, -0.3970],\n",
      "         [ 0.5298,  0.2795, -0.0898],\n",
      "         [ 0.9639,  0.8299,  0.8173],\n",
      "         [-0.4477,  0.5589, -0.0469]],\n",
      "\n",
      "        [[-0.4527, -0.1510, -1.2793],\n",
      "         [ 0.8157,  0.8164,  0.4771],\n",
      "         [-0.1934, -0.7526,  0.0624],\n",
      "         [-0.1545, -0.0899,  1.3320]]], grad_fn=<SqueezeBackward1>)\n",
      "Shape = torch.Size([2, 4, 3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Naturally to apply covolution, your input needs to be sequence. Eg: a sequence of words (= sentence)\n",
    "\n",
    "nn.Conv1d expects a batch input. And, it applies convolution filters(aka kernels) along the last dimension of input.\n",
    "What does this mean?\n",
    "\n",
    "To apply convolution on a single sentence, represented as a (l x n) matrix. Here l=sentence length; n=word embedding size\n",
    "1. Tranform it to a 'batched-input' of shape (1 x l x n) -- basically a batch of just 1 sentence\n",
    "2. We want to apply convolution filters along the length (2nd dimension). But nn.Conv1d assumes the 3rd dimension as the length.\n",
    "3. So transpose input to shape (1 x n x l), before convolving.\n",
    "\n",
    "To apply convolution on set of sentences, represented as a (s x l x n) matrix. Here s=no.of sentences; l=sentence length; n=word embedding size\n",
    "1. We already have a 'batched-input', no changes required\n",
    "2. We want to apply convolution filters along the length (2nd dimension) of each sentence. But for nn.Conv1d 3rd dimension is the sentence length.\n",
    "3. Transpose input to shape (s x n x l), before convolving.\n",
    "\"\"\"\n",
    "sent_embedded = sents_embedded[2]\n",
    "print_h4(\"Convolving single sentence '*{}*'\".format(sents_lang[2]))\n",
    "print('Embedded representation = {}'.format(sent_embedded))\n",
    "print('Shape = {}\\n'.format(sent_embedded.shape))\n",
    "\n",
    "# convert to 3-d 'batched-input', as required by nn.Conv1d\n",
    "# with 1st dimension being the batch size\n",
    "sent_embedded = sent_embedded.unsqueeze(0)\n",
    "print('Batched-input representation = {}'.format(sent_embedded))\n",
    "print('Shape = {}\\n'.format(sent_embedded.shape))\n",
    "\n",
    "# transpose\n",
    "sent_embedded = sent_embedded.transpose(dim0=1, dim1=2)\n",
    "print('Batched-input transposed = {}'.format(sent_embedded))\n",
    "print('Shape = {}\\n'.format(sent_embedded.shape))\n",
    "\n",
    "# convolve\n",
    "# output tensor's shape: (s x m x l_o)\n",
    "# Here s=batch_size; m=out_chanels (aka no.of filters/kernels); l_o=output length, which is a function of: input_length(l), stride (defaults to 1) & kernel_size\n",
    "cnlv_1 = c1d(sent_embedded)\n",
    "print('Convolution output = {}'.format(cnlv_1))\n",
    "print('Shape = {}\\n'.format(cnlv_1.shape))\n",
    "\n",
    "\n",
    "# now let's convolve a batch of 2 sentences\n",
    "print_h4(\"Convolving 2 sentences '*{}*' and '*{}*'\".format(sents_lang[1], sents_lang[3]))\n",
    "sents_embedded_2 = sents_embedded[torch.tensor([0, 1, 0, 1, 0], dtype=bool)]\n",
    "print('Batched-Input = {}'.format(sents_embedded_2))\n",
    "print('Shape = {}\\n'.format(sents_embedded_2.shape))\n",
    "\n",
    "# transpose\n",
    "sents_embedded_2 = sents_embedded_2.transpose(dim0=1, dim1=2)\n",
    "print('Batched-input transposed = {}'.format(sents_embedded_2))\n",
    "print('Shape = {}\\n'.format(sents_embedded_2.shape))\n",
    "\n",
    "# convolve\n",
    "# output tensor's shape: (s x m x l_o)\n",
    "# Here s=batch_size; m=out_chanels (aka no.of filters/kernels); l_o=output length, which is a function of: input_length(l), stride (defaults to 1) & kernel_size\n",
    "cnlv_2 = c1d(sents_embedded_2)\n",
    "print('Convolution output = {}'.format(cnlv_2))\n",
    "print('Shape = {}\\n'.format(cnlv_2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
