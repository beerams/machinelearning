{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usages of various PyTorch apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "def print_md(s):\n",
    "    display_markdown(s, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "[API Docs](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Sentences represented as array of indices"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*this is my book* => [13, 100, 88, 100]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*those are your books* => [61, 100, 78, 100]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*what is your name* => [11, 100, 78, 100]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*i will be back* => [100, 33, 98, 100]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*go out and about* => [69, 40, 70, 25]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Sentences represented as a tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 13, 100,  88, 100],\n",
      "        [ 61, 100,  78, 100],\n",
      "        [ 11, 100,  78, 100],\n",
      "        [100,  33,  98, 100],\n",
      "        [ 69,  40,  70,  25]])\n",
      "Shape = torch.Size([5, 4])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embedding of 'who'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3614,  0.4452,  0.5306, -1.2300,  0.5073,  0.5823, -0.1579, -0.8442,\n",
      "         -0.6339,  0.5294]], grad_fn=<EmbeddingBackward>)\n",
      "Shape = torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embedding of sentence those are your books"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3372, -2.1102, -0.2155, -1.3066,  1.3178,  0.0671,  0.8783, -0.1200,\n",
      "         -0.9483, -0.5488],\n",
      "        [-0.1346,  1.5501, -0.1412,  0.8763,  0.3140, -2.2132, -2.0929, -0.6477,\n",
      "          0.4682, -0.5573],\n",
      "        [-0.1762, -1.5421,  0.4677, -0.5787,  1.5979,  1.4621,  0.9051,  0.2137,\n",
      "          0.9754,  1.1032],\n",
      "        [-0.1346,  1.5501, -0.1412,  0.8763,  0.3140, -2.2132, -2.0929, -0.6477,\n",
      "          0.4682, -0.5573]], grad_fn=<EmbeddingBackward>)\n",
      "Shape = torch.Size([4, 10])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embeddings of all 5 sentences"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5163, -0.2413, -1.3965,  0.3270,  0.2376,  1.1178, -0.1212,\n",
      "          -1.2906, -0.7408,  1.6643],\n",
      "         [-0.1346,  1.5501, -0.1412,  0.8763,  0.3140, -2.2132, -2.0929,\n",
      "          -0.6477,  0.4682, -0.5573],\n",
      "         [ 0.5258, -1.7931, -0.9446,  0.9506, -0.7688, -0.3358, -0.5055,\n",
      "           0.8074,  0.1054,  1.6304],\n",
      "         [-0.1346,  1.5501, -0.1412,  0.8763,  0.3140, -2.2132, -2.0929,\n",
      "          -0.6477,  0.4682, -0.5573]],\n",
      "\n",
      "        [[ 0.3372, -2.1102, -0.2155, -1.3066,  1.3178,  0.0671,  0.8783,\n",
      "          -0.1200, -0.9483, -0.5488],\n",
      "         [-0.1346,  1.5501, -0.1412,  0.8763,  0.3140, -2.2132, -2.0929,\n",
      "          -0.6477,  0.4682, -0.5573],\n",
      "         [-0.1762, -1.5421,  0.4677, -0.5787,  1.5979,  1.4621,  0.9051,\n",
      "           0.2137,  0.9754,  1.1032],\n",
      "         [-0.1346,  1.5501, -0.1412,  0.8763,  0.3140, -2.2132, -2.0929,\n",
      "          -0.6477,  0.4682, -0.5573]],\n",
      "\n",
      "        [[-2.2525, -0.9693, -1.2082, -0.6579, -1.0667, -0.2031,  1.9947,\n",
      "           0.9070,  0.8579,  0.1010],\n",
      "         [-0.1346,  1.5501, -0.1412,  0.8763,  0.3140, -2.2132, -2.0929,\n",
      "          -0.6477,  0.4682, -0.5573],\n",
      "         [-0.1762, -1.5421,  0.4677, -0.5787,  1.5979,  1.4621,  0.9051,\n",
      "           0.2137,  0.9754,  1.1032],\n",
      "         [-0.1346,  1.5501, -0.1412,  0.8763,  0.3140, -2.2132, -2.0929,\n",
      "          -0.6477,  0.4682, -0.5573]],\n",
      "\n",
      "        [[-0.1346,  1.5501, -0.1412,  0.8763,  0.3140, -2.2132, -2.0929,\n",
      "          -0.6477,  0.4682, -0.5573],\n",
      "         [-0.4922, -1.3666, -2.1974, -0.0738, -0.4668,  1.2925, -0.1192,\n",
      "           0.5263, -1.4002,  0.4526],\n",
      "         [-0.7543,  0.4790, -0.8359,  1.0670,  0.7657, -0.8797,  1.9597,\n",
      "           1.2474,  1.9769,  1.1003],\n",
      "         [-0.1346,  1.5501, -0.1412,  0.8763,  0.3140, -2.2132, -2.0929,\n",
      "          -0.6477,  0.4682, -0.5573]],\n",
      "\n",
      "        [[ 0.2328,  0.6672, -0.3742,  0.7442, -0.8889,  1.0208,  3.4591,\n",
      "           0.3888,  0.8134, -0.8669],\n",
      "         [-0.4729,  0.8047,  0.6739,  1.2150, -0.0258, -0.9368,  0.8989,\n",
      "           0.0923,  1.6186,  0.6897],\n",
      "         [ 0.6994, -0.2631, -1.2692, -0.4627, -1.6694, -0.9504,  0.2991,\n",
      "           1.3278, -0.7132, -1.4496],\n",
      "         [-0.3081,  0.3290,  1.2129, -0.2923,  2.0744, -0.6394, -0.0174,\n",
      "           0.3849, -0.3353, -0.2518]]], grad_fn=<EmbeddingBackward>)\n",
      "Shape = torch.Size([5, 4, 10])\n"
     ]
    }
   ],
   "source": [
    "# This example illustrates the usage of nn.Embedding\n",
    "# Embedding is a lookup table -- to lookup a vector stored against a key (typically an integer index)\n",
    "# You supply a bunch of keys, you get back corresponding bunch of vectors\n",
    "\n",
    "# This example tries to lookup vectors corresponding to words in a sentence\n",
    "# Setup: here are 100 english words. Index of 'how' is 0, index of 'to' is 1, ... index of 'two' is 99\n",
    "v100 = ['how', 'to', 'her', 'at', 'up', 'see', 'in', 'thing', 'even', 'because', 'or', \n",
    "             'what', 'man', 'this', 'for', 'with', 'time', 'now', 'give', 'very', 'take', 'other', \n",
    "             'there', 'would', 'first', 'about', 'people', 'think', 'find', 'so', 'say', 'as', \n",
    "             'many', 'will', 'just', 'he', 'I', 'well', 'our', 'tell', 'out', 'have', 'can', 'its', \n",
    "             'make', 'get', 'if', 'than', 'use', 'that', 'new', 'also', 'from', 'by', 'his', 'year', \n",
    "             'do', 'some', 'the', 'no', 'a', 'those', 'she', 'come', 'one', 'their', 'more', \n",
    "             'these', 'all', 'go', 'and', 'could', 'him', 'into', 'only', 'who', 'of', 'it', 'your', \n",
    "             'not', 'you', 'here', 'when', 'on', 'which', 'then', 'know', 'them', 'my', 'me', 'we', \n",
    "             'want', 'they', 'like', 'look', 'day', 'way', 'but', 'be', 'two']\n",
    "\n",
    "# Naturally this vocabulary is insufficient even for most common sentences. Map all out of vocabulary words to <unk>\n",
    "# add <unk> to vocabulary at index 100. <unk> is the replacement for out of vocabulary words\n",
    "v100.append('<unk>')\n",
    "\n",
    "# create a dictionary to lookup word's index\n",
    "w2i = {word: i for i, word in enumerate(v100)}\n",
    "\n",
    "# define few sentences of fixed size. we wish to lookup embeddings for words in these sentences\n",
    "# here are 5 sentences\n",
    "sents_lang = [\n",
    "    'this is my book',\n",
    "    'those are your books',\n",
    "    'what is your name',\n",
    "    'i will be back',\n",
    "    'go out and about'\n",
    "]\n",
    "\n",
    "# tranform to indexed representation of sentences\n",
    "sents_indices = [[w2i.get(word, w2i['<unk>']) for word in sent.split()] for sent in sents_lang]\n",
    "\n",
    "# print representations\n",
    "print_md('#### Sentences represented as array of indices')\n",
    "for s1, s2 in zip(sents_lang, sents_indices):\n",
    "    print_md('*{}* => {}'.format(s1, s2))\n",
    "\n",
    "# convert sents_word_indices to a tensor\n",
    "sents_tensor = torch.tensor(sents_indices)\n",
    "print_md('#### Sentences represented as a tensor')\n",
    "print(sents_tensor)\n",
    "print('Shape = {}'.format(sents_tensor.shape))\n",
    "\n",
    "\n",
    "\n",
    "# create an embedding to lookup 10-dimensional vectors for each word\n",
    "embedding_1 = nn.Embedding(num_embeddings=len(v100), embedding_dim=10)\n",
    "\n",
    "# what's the embedding for the word 'who'\n",
    "embed = embedding_1(torch.tensor([ w2i['who']]))\n",
    "print_md(\"#### Embedding of 'who'\")\n",
    "print(\"{}\\nShape = {}\".format(embed, embed.shape))\n",
    "\n",
    "# what's the embedding of 2nd sentence\n",
    "embed = embedding_1(sents_tensor[1])\n",
    "print_md(\"#### Embedding of sentence {}\".format(sents_lang[1]))\n",
    "print(\"{}\\nShape = {}\".format(embed, embed.shape))\n",
    "\n",
    "# embeddings for all sentences\n",
    "embed = embedding_1(sents_tensor)\n",
    "print_md(\"#### Embeddings of all 5 sentences\")\n",
    "print(\"{}\\nShape = {}\".format(embed, embed.shape))\n",
    "\n",
    "\n",
    "# a little more sophisticated embedding lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
