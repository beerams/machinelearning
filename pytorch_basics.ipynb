{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usages of various PyTorch apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "def print_md(s):\n",
    "    display_markdown(s, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "[API Docs](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Sentences represented as array of indices"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*this is my book* => [13, 100, 88, 100]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*those are your books* => [61, 100, 78, 100]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*what is your name* => [11, 100, 78, 100]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*i will be back* => [100, 33, 98, 100]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*go out and about* => [69, 40, 70, 25]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Sentences represented as a tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 13, 100,  88, 100],\n",
      "        [ 61, 100,  78, 100],\n",
      "        [ 11, 100,  78, 100],\n",
      "        [100,  33,  98, 100],\n",
      "        [ 69,  40,  70,  25]])\n",
      "Shape = torch.Size([5, 4])\n"
     ]
    }
   ],
   "source": [
    "# This example illustrates the usage of nn.Embedding\n",
    "# Embedding is a lookup table -- to lookup a vector stored against a key (typically an integer index)\n",
    "# You supply a bunch of keys, you get back corresponding bunch of vectors\n",
    "\n",
    "# This example tries to lookup vectors corresponding to words in a sentence\n",
    "# Setup: here are 100 english words. Index of 'how' is 0, index of 'to' is 1, ... index of 'two' is 99\n",
    "v100 = ['how', 'to', 'her', 'at', 'up', 'see', 'in', 'thing', 'even', 'because', 'or', \n",
    "             'what', 'man', 'this', 'for', 'with', 'time', 'now', 'give', 'very', 'take', 'other', \n",
    "             'there', 'would', 'first', 'about', 'people', 'think', 'find', 'so', 'say', 'as', \n",
    "             'many', 'will', 'just', 'he', 'I', 'well', 'our', 'tell', 'out', 'have', 'can', 'its', \n",
    "             'make', 'get', 'if', 'than', 'use', 'that', 'new', 'also', 'from', 'by', 'his', 'year', \n",
    "             'do', 'some', 'the', 'no', 'a', 'those', 'she', 'come', 'one', 'their', 'more', \n",
    "             'these', 'all', 'go', 'and', 'could', 'him', 'into', 'only', 'who', 'of', 'it', 'your', \n",
    "             'not', 'you', 'here', 'when', 'on', 'which', 'then', 'know', 'them', 'my', 'me', 'we', \n",
    "             'want', 'they', 'like', 'look', 'day', 'way', 'but', 'be', 'two']\n",
    "\n",
    "# Naturally this vocabulary is insufficient even for most common sentences. Map all out of vocabulary words to <unk>\n",
    "# add <unk> to vocabulary at index 100. <unk> is the replacement for out of vocabulary words\n",
    "v100.append('<unk>')\n",
    "\n",
    "# create a dictionary to lookup word's index\n",
    "w2i = {word: i for i, word in enumerate(v100)}\n",
    "\n",
    "# define few sentences of fixed size. we wish to lookup embeddings for words in these sentences\n",
    "# here are 5 sentences\n",
    "sents_lang = [\n",
    "    'this is my book',\n",
    "    'those are your books',\n",
    "    'what is your name',\n",
    "    'i will be back',\n",
    "    'go out and about'\n",
    "]\n",
    "\n",
    "# tranform to indexed representation of sentences\n",
    "sents_indices = [[w2i.get(word, w2i['<unk>']) for word in sent.split()] for sent in sents_lang]\n",
    "\n",
    "# print representations\n",
    "print_md('#### Sentences represented as array of indices')\n",
    "for s1, s2 in zip(sents_lang, sents_indices):\n",
    "    print_md('*{}* => {}'.format(s1, s2))\n",
    "\n",
    "# convert sents_word_indices to a tensor\n",
    "sents_tensor = torch.tensor(sents_indices)\n",
    "print_md('#### Sentences represented as a tensor')\n",
    "print(sents_tensor)\n",
    "print('Shape = {}'.format(sents_tensor.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an embedding to lookup 10-dimensional vectors for each word\n",
    "embedding_1 = nn.Embedding(num_embeddings=len(v100), embedding_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Embedding weights"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weights = torch.Size([101, 10])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embedding of 'who'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1648, -0.4181,  0.8845,  0.8224,  0.5519, -0.2669, -0.7117,  0.2081,\n",
      "          1.8748, -0.4355]], grad_fn=<EmbeddingBackward>)\n",
      "Shape = torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embedding of the sentence: *those are your books*"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1838,  0.7782, -0.0468, -0.3376,  0.1745, -0.5339, -0.4489, -1.4725,\n",
      "         -1.1806,  0.5460],\n",
      "        [-0.7481, -0.3882, -1.2286,  2.0887, -1.7087,  1.2943,  1.5759, -1.4993,\n",
      "         -0.9034, -0.8589],\n",
      "        [ 2.6125, -0.5519,  1.5691, -2.0202,  0.7104, -0.7424,  0.3529,  1.6167,\n",
      "          2.0495, -0.3036],\n",
      "        [-0.7481, -0.3882, -1.2286,  2.0887, -1.7087,  1.2943,  1.5759, -1.4993,\n",
      "         -0.9034, -0.8589]], grad_fn=<EmbeddingBackward>)\n",
      "Shape = torch.Size([4, 10])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embeddings of all 5 sentences"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.2721, -0.1471, -0.0531,  0.1630,  0.5624,  1.1155,  0.7727,\n",
      "          -0.0911, -1.0680, -1.3912],\n",
      "         [-0.7481, -0.3882, -1.2286,  2.0887, -1.7087,  1.2943,  1.5759,\n",
      "          -1.4993, -0.9034, -0.8589],\n",
      "         [-1.8170, -0.6491,  0.5852, -1.1165,  0.3322,  0.8839,  1.2945,\n",
      "          -1.7932,  1.3616,  0.2572],\n",
      "         [-0.7481, -0.3882, -1.2286,  2.0887, -1.7087,  1.2943,  1.5759,\n",
      "          -1.4993, -0.9034, -0.8589]],\n",
      "\n",
      "        [[-0.1838,  0.7782, -0.0468, -0.3376,  0.1745, -0.5339, -0.4489,\n",
      "          -1.4725, -1.1806,  0.5460],\n",
      "         [-0.7481, -0.3882, -1.2286,  2.0887, -1.7087,  1.2943,  1.5759,\n",
      "          -1.4993, -0.9034, -0.8589],\n",
      "         [ 2.6125, -0.5519,  1.5691, -2.0202,  0.7104, -0.7424,  0.3529,\n",
      "           1.6167,  2.0495, -0.3036],\n",
      "         [-0.7481, -0.3882, -1.2286,  2.0887, -1.7087,  1.2943,  1.5759,\n",
      "          -1.4993, -0.9034, -0.8589]],\n",
      "\n",
      "        [[ 0.3513,  0.0174, -0.4158, -1.2374, -1.3397,  0.5142, -0.2744,\n",
      "           0.8178, -0.2960,  2.0155],\n",
      "         [-0.7481, -0.3882, -1.2286,  2.0887, -1.7087,  1.2943,  1.5759,\n",
      "          -1.4993, -0.9034, -0.8589],\n",
      "         [ 2.6125, -0.5519,  1.5691, -2.0202,  0.7104, -0.7424,  0.3529,\n",
      "           1.6167,  2.0495, -0.3036],\n",
      "         [-0.7481, -0.3882, -1.2286,  2.0887, -1.7087,  1.2943,  1.5759,\n",
      "          -1.4993, -0.9034, -0.8589]],\n",
      "\n",
      "        [[-0.7481, -0.3882, -1.2286,  2.0887, -1.7087,  1.2943,  1.5759,\n",
      "          -1.4993, -0.9034, -0.8589],\n",
      "         [-1.7627,  1.6661,  0.4635,  1.2396,  0.1963, -1.7206,  0.8836,\n",
      "           1.3458,  1.5276, -0.9787],\n",
      "         [ 0.0492,  0.1535,  1.0895,  0.1093, -1.7204, -0.4438, -0.9846,\n",
      "          -0.6936,  0.7615,  0.6679],\n",
      "         [-0.7481, -0.3882, -1.2286,  2.0887, -1.7087,  1.2943,  1.5759,\n",
      "          -1.4993, -0.9034, -0.8589]],\n",
      "\n",
      "        [[-1.0535, -0.2433,  0.0855,  0.0229, -0.9258,  0.3068, -0.6174,\n",
      "           0.0925, -0.0506,  1.4886],\n",
      "         [-0.6829, -0.6449,  0.3054, -0.0227,  0.1939,  0.2198,  0.1927,\n",
      "           0.7756,  2.0519, -0.3287],\n",
      "         [ 1.0004, -1.1559,  0.7329,  0.3963,  1.0697,  0.3044,  0.1086,\n",
      "           0.0443,  1.2831,  0.2687],\n",
      "         [ 0.9654,  1.2845, -1.5028,  2.3455,  0.3844, -0.3434, -1.3868,\n",
      "          -1.2242,  0.0039, -0.3819]]], grad_fn=<EmbeddingBackward>)\n",
      "Shape = torch.Size([5, 4, 10])\n"
     ]
    }
   ],
   "source": [
    "# underneath embeddings are weights, initialized with random values ~ N(0, 1)\n",
    "print_md(\"#### Embedding weights\")\n",
    "print(\"Shape of weights = {}\".format(embedding_1.weight.shape))\n",
    "\n",
    "# what's the embedding for the word 'who'\n",
    "embed = embedding_1(torch.tensor([ w2i['who']]))\n",
    "print_md(\"#### Embedding of 'who'\")\n",
    "print(\"{}\\nShape = {}\".format(embed, embed.shape))\n",
    "\n",
    "# what's the embedding of 2nd sentence\n",
    "embed = embedding_1(sents_tensor[1])\n",
    "print_md(\"#### Embedding of the sentence: *{}*\".format(sents_lang[1]))\n",
    "print(\"{}\\nShape = {}\".format(embed, embed.shape))\n",
    "\n",
    "# embeddings for all sentences\n",
    "embed = embedding_1(sents_tensor)\n",
    "print_md(\"#### Embeddings of all 5 sentences\")\n",
    "print(\"{}\\nShape = {}\".format(embed, embed.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little sophisticated embedding layer\n",
    "# Suppose the embeddings we want to lookup are to be linear transformed. This can be done by implementing a custom module wrapping nn.Embedding\n",
    "# This module applies a linear transformation on word embeddings of size 'in_embedding_dim', the output is embeddings of size 'out_embedding_dim'\n",
    "class CustomEmbedding1(nn.Module):\n",
    "    def __init__(self, num_embeddings, in_embedding_dim, out_embedding_dim):\n",
    "        super(CustomEmbedding1, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, in_embedding_dim)\n",
    "        self.linear = nn.Linear(in_features=in_embedding_dim, out_features=out_embedding_dim)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.linear(self.embedding(input))\n",
    "\n",
    "cust_embedding_1 = CustomEmbedding1(num_embeddings=len(v100), in_embedding_dim=10, out_embedding_dim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Embedding of 'who'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4906, -0.3392, -0.9271,  0.0678,  1.0022]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "Shape = torch.Size([1, 5])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embedding of the sentence: *those are your books*"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4538,  0.8740,  1.2084, -0.6495, -0.9214],\n",
      "        [-0.2463,  0.0895,  0.8403,  0.3683,  0.4483],\n",
      "        [ 0.5683,  0.5978,  0.2839, -0.7738, -0.9881],\n",
      "        [-0.2463,  0.0895,  0.8403,  0.3683,  0.4483]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "Shape = torch.Size([4, 5])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Embeddings of all 5 sentences"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5852,  1.3370,  0.3591, -0.1961, -0.7762],\n",
      "         [-0.2463,  0.0895,  0.8403,  0.3683,  0.4483],\n",
      "         [-0.5432,  0.5927, -0.5425,  0.3453,  0.5448],\n",
      "         [-0.2463,  0.0895,  0.8403,  0.3683,  0.4483]],\n",
      "\n",
      "        [[ 0.4538,  0.8740,  1.2084, -0.6495, -0.9214],\n",
      "         [-0.2463,  0.0895,  0.8403,  0.3683,  0.4483],\n",
      "         [ 0.5683,  0.5978,  0.2839, -0.7738, -0.9881],\n",
      "         [-0.2463,  0.0895,  0.8403,  0.3683,  0.4483]],\n",
      "\n",
      "        [[-0.4582,  0.4004,  0.3303,  0.7274,  0.0556],\n",
      "         [-0.2463,  0.0895,  0.8403,  0.3683,  0.4483],\n",
      "         [ 0.5683,  0.5978,  0.2839, -0.7738, -0.9881],\n",
      "         [-0.2463,  0.0895,  0.8403,  0.3683,  0.4483]],\n",
      "\n",
      "        [[-0.2463,  0.0895,  0.8403,  0.3683,  0.4483],\n",
      "         [-0.0974, -0.7010, -0.3145,  0.3956,  0.6554],\n",
      "         [-0.9131,  0.4701,  0.1388,  0.7826,  1.1020],\n",
      "         [-0.2463,  0.0895,  0.8403,  0.3683,  0.4483]],\n",
      "\n",
      "        [[ 0.1010,  1.1152, -0.2566, -0.6424, -0.5444],\n",
      "         [-0.0081,  0.4487,  0.3053, -0.3266, -0.2560],\n",
      "         [-0.2497,  0.5982, -0.2606, -0.3743,  0.5047],\n",
      "         [-1.0944,  0.0944,  0.1379,  0.5292, -0.4469]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Shape = torch.Size([5, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "# what's the embedding for the word 'who'\n",
    "cust_embed = cust_embedding_1(torch.tensor([ w2i['who']]))\n",
    "print_md(\"#### Embedding of 'who'\")\n",
    "print(\"{}\\nShape = {}\".format(cust_embed, cust_embed.shape))\n",
    "\n",
    "# what's the embedding of 2nd sentence\n",
    "cust_embed = cust_embedding_1(sents_tensor[1])\n",
    "print_md(\"#### Embedding of the sentence: *{}*\".format(sents_lang[1]))\n",
    "print(\"{}\\nShape = {}\".format(cust_embed, cust_embed.shape))\n",
    "\n",
    "# embeddings for all sentences\n",
    "cust_embed = cust_embedding_1(sents_tensor)\n",
    "print_md(\"#### Embeddings of all 5 sentences\")\n",
    "print(\"{}\\nShape = {}\".format(cust_embed, cust_embed.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
