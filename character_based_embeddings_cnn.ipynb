{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An illustration of character-based embedding lookup using convolution layer\n",
    "\n",
    "In many NLP tasks word are resolved to embedding vectors, via a simple lookup on an embedding layer. The embedding layer may be initialized via pre-trained word vectors such as word2vec or may be randomly initialized and word embeddings are learnt in training phase.\n",
    "\n",
    "In this illustration, word embedding vectors are composed from character-level embeddings. The process is as follows.\n",
    "After pre-processing sentences to ensure all sentences and words are of equal length.\n",
    "1. For each word, lookup embeddings for each character of the word\n",
    "2. Apply 1-D convolution layer to create features\n",
    "3. Apply max pooling to get concise feature representation, which will serve as the word's embedding\n",
    "\n",
    "Computed word embeddings can be fed into larger, deeper networks for NLP tasks such as NMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ipython_utils import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Character indices"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25, 'a': 26, 'b': 27, 'c': 28, 'd': 29, 'e': 30, 'f': 31, 'g': 32, 'h': 33, 'i': 34, 'j': 35, 'k': 36, 'l': 37, 'm': 38, 'n': 39, 'o': 40, 'p': 41, 'q': 42, 'r': 43, 's': 44, 't': 45, 'u': 46, 'v': 47, 'w': 48, 'x': 49, 'y': 50, 'z': 51, '0': 52, '1': 53, '2': 54, '3': 55, '4': 56, '5': 57, '6': 58, '7': 59, '8': 60, '9': 61, '<pad>': 62, '<unk>': 63}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Random batch of sentences"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Sentence | # words|\n",
       "|--|--|\n",
       "| Pluto, Makemake, and Ceres are dwarf planets | 7 |\n",
       "| Aurora Borealis is a spectacle! | 5 |\n",
       "| To be or not to be | 6 |\n",
       "| Light at the end of the tunnel. | 7 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Sentence | Sentence with word indices|\n",
       "|--|--|\n",
       "| Pluto, Makemake, and Ceres are dwarf planets | [[15, 37, 46, 45, 40, 63, 62, 62, 62, 62, 62, 62, 62, 62, 62], [12, 26, 36, 30, 38, 26, 36, 30, 63, 62, 62, 62, 62, 62, 62], [26, 39, 29, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62], [2, 30, 43, 30, 44, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62], [26, 43, 30, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62], [29, 48, 26, 43, 31, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62], [41, 37, 26, 39, 30, 45, 44, 62, 62, 62, 62, 62, 62, 62, 62]] |\n",
       "| Aurora Borealis is a spectacle! | [[0, 46, 43, 40, 43, 26, 62, 62, 62, 62, 62, 62, 62, 62, 62], [1, 40, 43, 30, 26, 37, 34, 44, 62, 62, 62, 62, 62, 62, 62], [34, 44, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62], [26, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62], [44, 41, 30, 28, 45, 26, 28, 37, 30, 63, 62, 62, 62, 62, 62], [62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62], [62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62]] |\n",
       "| To be or not to be | [[19, 40, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62], [27, 30, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62], [40, 43, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62], [39, 40, 45, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62], [45, 40, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62], [27, 30, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62], [62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62]] |\n",
       "| Light at the end of the tunnel. | [[11, 34, 32, 33, 45, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62], [26, 45, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62], [45, 33, 30, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62], [30, 39, 29, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62], [40, 31, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62], [45, 33, 30, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62], [45, 46, 39, 39, 30, 37, 63, 62, 62, 62, 62, 62, 62, 62, 62]] |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# constants\n",
    "BATCH_SIZE = 4\n",
    "MAX_WORD_LEN = 15  # this constant is used to ensure all words are of this length, pad/truncate shorter/longer words respectively\n",
    "\n",
    "# Setup miniaturized dataset for the purpose of illustration\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "\n",
    "# dictionary to lookup an index given a character\n",
    "char2index = {ch: i for i, ch in enumerate(alphabet)}\n",
    "\n",
    "# add <pad> and <unk> token for padding character and out-of-alphabet caracters\n",
    "char2index['<pad>'] = len(char2index)\n",
    "char2index['<unk>'] = len(char2index)\n",
    "\n",
    "# define constants for padding and unk characters\n",
    "PAD_TOKEN = char2index['<pad>']\n",
    "UNK_TOKEN = char2index['<unk>']\n",
    "\n",
    "print_h4('Character indices')\n",
    "print(char2index)\n",
    "\n",
    "# few sentences\n",
    "sents_lang = [\n",
    "    'To be or not to be',\n",
    "    'Programming is fun',\n",
    "    'Light at the end of the tunnel.',\n",
    "    'Aurora Borealis is a spectacle!',\n",
    "    'Mercury, Venus, Earth and Mars are rocky planets',\n",
    "    'Jupiter and Saturn are gas giants',\n",
    "    'Uranus and Neptune are ice giants',\n",
    "    'Pluto, Makemake, and Ceres are dwarf planets'\n",
    "]\n",
    "\n",
    "# strip out leading/trailing whitespaces, if any\n",
    "sents_lang = [sent.strip() for sent in sents_lang]\n",
    "\n",
    "# select a random batch of sentences\n",
    "random.shuffle(sents_lang)\n",
    "sents_lang_batch = sents_lang[0:BATCH_SIZE]\n",
    "\n",
    "print_h4('Random batch of sentences')\n",
    "print_table(['Sentence', '# words'], [(sent, len(sent.split())) for sent in sents_lang_batch])\n",
    "\n",
    "# convert sentences to a list of list of words\n",
    "sents_lang_batch_words = [sent.split() for sent in sents_lang_batch]\n",
    "\n",
    "# convert each word to an array of indices\n",
    "sent_batch = [[[char2index.get(ch, UNK_TOKEN) for ch in word] for word in sent] for sent in sents_lang_batch_words]\n",
    "\n",
    "# now make sure all sentences are of the same length and in each sentence, make sure all words are of the same length. insert <pad> characters where necessary\n",
    "max_sent_len = max(len(sent) for sent in sent_batch)\n",
    "for s in range(len(sent_batch)):\n",
    "    sent = sent_batch[s]\n",
    "    for w in range(len(sent)):\n",
    "        # pad/truncate each word as necessary\n",
    "        if len(sent[w]) < MAX_WORD_LEN:\n",
    "            sent[w].extend([PAD_TOKEN]*(MAX_WORD_LEN - len(sent[w])))\n",
    "        elif len(sent[w]) > MAX_WORD_LEN:\n",
    "            sent[w] = sent[w][0:MAX_WORD_LEN]\n",
    "\n",
    "    # pad sentence with extra words to ensure all sentences are of same length\n",
    "    if len(sent) < max_sent_len:\n",
    "        sent.extend([[PAD_TOKEN]*MAX_WORD_LEN]*(max_sent_len-len(sent)))\n",
    "\n",
    "\n",
    "print_table(['Sentence', 'Sentence with word indices'], zip(sents_lang, sent_batch))\n",
    "\n",
    "# validate to ensure correct sizes\n",
    "for sent in sent_batch:\n",
    "    assert len(sent) == max_sent_len\n",
    "    for word in sent:\n",
    "        assert len(word) == MAX_WORD_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
